from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random
from tqdm import trange

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from utils import Language, RecipeData, collate_fn, build_language

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TEACHER_FORCING = 1
learning_rate = 0.001
dataset_path = "pass"
BATCH_SIZE = 8
NUM_EPOCHS = 50
hidden_dimensions = 256
encoder_dropout = 0.5
decoder_dropout = 0.5

class Encoder(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_size, dropout):
        super().__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_dim, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_size, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, source):
        embeds = self.dropout(self.embedding)
        outputs, (hidden, cell) = self.lstm(embeds)
        return hidden, cell
    
    def init_hidden(self):
        return torch.zeros(self.hidden_size)
    
class Decoder(nn.Module):
    def __init__(self, output_dim, embed_dim, hidden_size, dropout) -> None:
        super().__init__()
        self.output_dim = output_dim
        self.embed_dim = embed_dim
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_dim, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_size, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden, cell):
        inputs = input.unsqueeze(0)
        embeds = self.dropout(self.embedding(inputs))
        outputs, (hidden, cell) = self.lstm(embeds, (hidden, cell))
        prediction = self.fc(outputs.squeeze(0))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder) -> None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        assert encoder.hidden_size == decoder.hidden_size

    def forward(source, target, tfr):
        batch_size = target.shape[1]
        target_len = target.shape[0]
        target_vocab_size = self.decoder.output_dim
        outputs = torch.zeros(target_len, batch_size, target_vocab_size)
        hidden, cell = self.encoder(source)
        decoder_input = target[0, :]
        for token in range(1,target_len):
            output, hidden, cell = self.decoder(decoder_input, hidden, cell)
            outputs[token] = output
            teacher_force = random.random < tfr
            decoder_input = target[t] if teacher_force else output.argmax(1)
        return outputs
    

ingredient_vocabulary, recipe_vocabulary = build_language(dataset_path)
input_dimensions = len(ingredient_vocabulary)
output_dimensions = len(recipe_vocabulary)


encoder = Encoder(input_dim=input_dimensions, embed_dim=hidden_dimensions, hidden_size=hidden_dimensions, dropout=encoder_dropout)
decoder = Decoder(output_dim=output_dimensions, embed_dim=hidden_dimensions, hidden_size=hidden_dimensions, dropout=decoder_dropout)
model = Seq2Seq(encoder, decoder)

training_data = RecipeData(path="Cooking_Data/train", ing_lang=ingredient_vocabulary, instr_lang=recipe_vocabulary)

train_loader = DataLoader(data=training_data,batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

loss = nn.NLLLoss()

def run_epoch(model, dataloader, optimiser, criterion):
    model.train()
    epoch_loss = 0
    for ingredients, steps in dataloader:
        optimiser.zero_grad()
        output = model(ingredients, steps, TEACHER_FORCING)
        out_dim = output.shape[-1]
        out = output[1:].view(-1, out_dim) 
        steps = steps[1:].view(-1)
        loss = criterion(out, steps)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
        optimiser.step()
        epoch_loss += loss.item()
    return epoch_loss/len(dataloader)

def validate(model, dataloader, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for ingredients, steps in dataloader:
            output = model(ingredients, steps, TEACHER_FORCING)
            out_dim = output.shape[-1]
            out = output[1:].view(-1, out_dim) 
            steps = steps[1:].view(-1)
            loss = criterion(out, steps)
            epoch_loss += loss.item()
    return epoch_loss/len(dataloader)

def train(model, dataloader, optimiser, criterion, num_epochs):
    for epoch in trange(num_epochs):
        train_loss = run_epoch(model, dataloader, optimiser, criterion)
        validation_loss = validate(model, dataloader, criterion)
        print("Training Accuracy: {}  |   Validation Accuracy: {}".format(train_loss,validation_loss))

    

#how do we need to set up the complete vocabularies?
